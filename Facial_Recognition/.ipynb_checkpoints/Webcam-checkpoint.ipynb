{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zoomate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "IMAGE PROCESSING\n",
    "'''\n",
    "import sys\n",
    "import argparse\n",
    "import cv2\n",
    "from libfaceid.detector import FaceDetectorModels, FaceDetector\n",
    "from libfaceid.encoder  import FaceEncoderModels, FaceEncoder\n",
    "from libfaceid.pose import FacePoseEstimatorModels, FacePoseEstimator\n",
    "from libfaceid.age import FaceAgeEstimatorModels, FaceAgeEstimator\n",
    "from libfaceid.gender import FaceGenderEstimatorModels, FaceGenderEstimator\n",
    "from libfaceid.emotion import FaceEmotionEstimatorModels, FaceEmotionEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STREAMING\n",
    "'''\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, show, push_notebook\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STREAMING CONTROL\n",
    "'''\n",
    "import threading\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "BUTTON CONTROLS\n",
    "'''\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Button, HBox, VBox\n",
    "streaming_toggle = widgets.ToggleButton(\n",
    "                        value=False,\n",
    "                        description='Start Streaming',\n",
    "                        disabled=False,\n",
    "                        button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
    "                        tooltip='Streaming',\n",
    "                        icon='play'\n",
    "                    )\n",
    "\n",
    "show_result_button = widgets.Button(description=\"Show Result\")\n",
    "show_result_button.on_click(on_button_clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR_DATASET               = \"datasets\"\n",
    "INPUT_DIR_MODEL_DETECTION       = \"models/detection/\"\n",
    "INPUT_DIR_MODEL_ENCODING        = \"models/encoding/\"\n",
    "INPUT_DIR_MODEL_TRAINING        = \"models/training/\"\n",
    "INPUT_DIR_MODEL_ESTIMATION      = \"models/estimation/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOLUTION_QVGA   = (320, 240)\n",
    "RESOLUTION_VGA    = (640, 480)\n",
    "RESOLUTION_HD     = (1280, 720)\n",
    "RESOLUTION_FULLHD = (1920, 1080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STREAMING HELPER FUNCTIONS\n",
    "'''\n",
    "def image_resize(image, width = None, height = None, inter = cv2.INTER_AREA):\n",
    "    # initialize the dimensions of the image to be resized and\n",
    "    # grab the image size\n",
    "    dim = None\n",
    "    (h, w) = image.shape[:2]\n",
    "\n",
    "    # if both the width and height are None, then return the\n",
    "    # original image\n",
    "    if width is None and height is None:\n",
    "        return image\n",
    "\n",
    "    # check to see if the width is None\n",
    "    if width is None:\n",
    "        # calculate the ratio of the height and construct the\n",
    "        # dimensions\n",
    "        r = height / float(h)\n",
    "        dim = (int(w * r), height)\n",
    "\n",
    "    # otherwise, the height is None\n",
    "    else:\n",
    "        # calculate the ratio of the width and construct the\n",
    "        # dimensions\n",
    "        r = width / float(w)\n",
    "        dim = (width, int(h * r))\n",
    "\n",
    "    # resize the image\n",
    "    resized = cv2.resize(image, dim, interpolation = inter)\n",
    "\n",
    "    # return the resized image\n",
    "    return resized\n",
    "\n",
    "def show_init_screen():\n",
    "    image_data = cv2.imread(\"init_screen.jpeg\")\n",
    "    image_data = cv2.cvtColor(image_data, cv2.COLOR_BGR2RGBA)\n",
    "    image_data = cv2.flip(image_data, 0)\n",
    "    myImage.data_source.data['image']=[image_data]\n",
    "    push_notebook()\n",
    "\n",
    "def cam_init(cam_index, width, height): \n",
    "    cap = cv2.VideoCapture(cam_index)\n",
    "    if sys.version_info < (3, 0):\n",
    "        cap.set(cv2.cv.CV_CAP_PROP_FPS, 30)\n",
    "        cap.set(cv2.cv.CV_CAP_PROP_FRAME_WIDTH,  width)\n",
    "        cap.set(cv2.cv.CV_CAP_PROP_FRAME_HEIGHT, height)\n",
    "    else:\n",
    "        cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH,  width)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "    return cap\n",
    "\n",
    "def label_face(frame, face_rect, face_id, confidence):\n",
    "    (x, y, w, h) = face_rect\n",
    "    cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 255, 255), 1)\n",
    "    if face_id is not None:\n",
    "        cv2.putText(frame, \"{} {:.2f}%\".format(face_id, confidence), \n",
    "            (x+5,y+h-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()\n",
    "emotions = []\n",
    "\n",
    "cam_index = 0\n",
    "cam_resolution = RESOLUTION_QVGA\n",
    "model_detector=FaceDetectorModels.HAARCASCADE\n",
    "model_poseestimator=FacePoseEstimatorModels.DEFAULT\n",
    "model_ageestimator=FaceAgeEstimatorModels.DEFAULT\n",
    "model_genderestimator=FaceGenderEstimatorModels.DEFAULT\n",
    "model_emotionestimator=FaceEmotionEstimatorModels.DEFAULT\n",
    "\n",
    "\n",
    "# Initialize the camera\n",
    "camera = cam_init(cam_index, cam_resolution[0], cam_resolution[1])\n",
    "\n",
    "ret, frame = camera.read()\n",
    "frame=cv2.cvtColor(frame, cv2.COLOR_BGR2RGBA) # because Bokeh expects a RGBA image\n",
    "frame=cv2.flip(frame, 0) # because Bokeh flips vertically\n",
    "\n",
    "width=frame.shape[1]\n",
    "height=frame.shape[0]\n",
    "p = figure(x_range=(0,width), y_range=(0,height), output_backend=\"webgl\", width=width, height=height)\n",
    "myImage = p.image_rgba(image=[frame], x=0, y=0, dw=width, dh=height)\n",
    "show(p, notebook_handle=True)\n",
    "\n",
    "print(\"Camera Reesolution: {0}X{1}\".format(width, height))\n",
    "push_notebook()\n",
    "\n",
    "\n",
    "try:\n",
    "    # Initialize face detection\n",
    "    face_detector = FaceDetector(model=model_detector, path=INPUT_DIR_MODEL_DETECTION)#, optimize=True)\n",
    "    # Initialize face pose/age/gender estimation\n",
    "    face_pose_estimator = FacePoseEstimator(model=model_poseestimator, path=INPUT_DIR_MODEL_ESTIMATION)\n",
    "    face_age_estimator = FaceAgeEstimator(model=model_ageestimator, path=INPUT_DIR_MODEL_ESTIMATION)\n",
    "    face_gender_estimator = FaceGenderEstimator(model=model_genderestimator, path=INPUT_DIR_MODEL_ESTIMATION)\n",
    "    face_emotion_estimator = FaceEmotionEstimator(model=model_emotionestimator, path=INPUT_DIR_MODEL_ESTIMATION)\n",
    "except:\n",
    "    print(\"Warning, check if models and trained dataset models exists!\")\n",
    "(age, gender, emotion) = (None, None, None)\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    # Capture frame from webcam\n",
    "    ret, frame = camera.read()\n",
    "    if frame is None:\n",
    "        print(\"Error, check if camera is connected!\")\n",
    "        break\n",
    "\n",
    "    # Detect and identify faces in the frame\n",
    "    faces = face_detector.detect(frame)\n",
    "    for (index, face) in enumerate(faces):\n",
    "        (x, y, w, h) = face\n",
    "\n",
    "        # Detect age, gender, emotion\n",
    "        face_image = frame[y:y+h, h:h+w]\n",
    "        age = face_age_estimator.estimate(frame, face_image)\n",
    "        gender = face_gender_estimator.estimate(frame, face_image)\n",
    "        emotion = face_emotion_estimator.estimate(frame, face_image)\n",
    "        emotions.append(emotion)\n",
    "\n",
    "        # Detect and draw face pose locations\n",
    "        shape = face_pose_estimator.detect(frame, face)\n",
    "        face_pose_estimator.add_overlay(frame, shape)\n",
    "\n",
    "        # Display age, gender, emotion\n",
    "\n",
    "    cv2.putText(frame, \"Age: {}\".format(age), \n",
    "        (x, y-45), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "    cv2.putText(frame, \"Gender: {}\".format(gender), \n",
    "        (x, y-30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "    cv2.putText(frame, \"Emotion: {}\".format(emotion), \n",
    "        (x, y-15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "        \n",
    "    # Display updated frame to Jupyter Notebook\n",
    "    frame=cv2.cvtColor(frame, cv2.COLOR_BGR2RGBA) # because Bokeh expects a RGBA image\n",
    "    frame=cv2.flip(frame, 0) # because Bokeh flips vertically\n",
    "\n",
    "\n",
    "    myImage.data_source.data['image']=[frame]\n",
    "    push_notebook()\n",
    "    time.sleep(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from collections import Counter\n",
    "def on_button_clicked(b):\n",
    "    letter_counts = Counter(emotions)\n",
    "    df = pandas.DataFrame.from_dict(letter_counts, orient='index')\n",
    "    df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(show_result_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "hide_input": true,
  "kernelspec": {
   "display_name": "psyc",
   "language": "python",
   "name": "psyc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
